# -*- coding: utf-8 -*-
"""CodeAlpha Intership 2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JxlBTFkEaUydpfxu0L13aRC6M4mgmdVy

TASK 1 - IRIS FLOWER CLASSIFICATION
"""

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Load the dataset
df = pd.read_csv('/content/Iris.csv')  # Make sure the file is in the same directory

# Step 2: Explore the data
print(df.head())
print(df.describe())
print(df['Species'].value_counts())

# Step 3: Drop unnecessary columns if any
df.drop('Id', axis=1, inplace=True)

# Step 4: Encode target labels
le = LabelEncoder()
df['Species'] = le.fit_transform(df['Species'])  # Setosa=0, Versicolor=1, Virginica=2

# Step 5: Split into features and labels
X = df.drop('Species', axis=1)
y = df['Species']

# Step 6: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 7: Train a classification model (Random Forest used here)
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Step 8: Predict on test set
y_pred = model.predict(X_test)

# Step 9: Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Step 10: Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""TASK 2 - UNEMPLOYMENT ANALYSIS WITH PYTHON"""

# Step 1: Import libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Step 2: Load the dataset
df1 = pd.read_csv("/content/Unemployment_Rate_upto_11_2020.csv")

# Step 3: Inspect data
print(df1.head())
print(df1.info())
print(df1.isnull().sum())

df1.columns = df1.columns.str.strip()
df1['Date'] = pd.to_datetime(df1['Date'], dayfirst=True)
df1 = df1.rename(columns={
    'Estimated Unemployment Rate (%)': 'Unemployment_Rate',
    'Estimated Employed': 'Employed',
    'Estimated Labour Participation Rate (%)': 'Labour_Participation_Rate'
})

# Step 6: National-level trend
df_national = df1.groupby('Date')[
    ['Unemployment_Rate', 'Employed', 'Labour_Participation_Rate']
].mean().reset_index()

plt.figure(figsize=(10, 5))
sns.lineplot(data=df_national, x='Date', y='Unemployment_Rate', color='red')
plt.title("National Unemployment Trend During COVID-19")
plt.xlabel("Date")
plt.ylabel("Unemployment Rate (%)")
plt.grid(True)
plt.tight_layout()
plt.show()

# Step 7: Month-wise average unemployment
df1['Month'] = df1['Date'].dt.month
monthly_avg = df1.groupby('Month')['Unemployment_Rate'].mean()

plt.figure(figsize=(8, 4))
monthly_avg.plot(kind='bar', color='orange')
plt.title("Average Monthly Unemployment Rate")
plt.ylabel("Unemployment Rate (%)")
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# Ensure column names are stripped and consistent
df1.columns = df1.columns.str.strip()

# Renaming
df1 = df1.rename(columns={
    'Estimated Unemployment Rate (%)': 'Unemployment_Rate',
    'Region': 'Region',  # Ensure Region exists
    'Date': 'Date'
})

# Convert date
df1['Date'] = pd.to_datetime(df1['Date'], dayfirst=True)

# Now plot
g = sns.FacetGrid(df1, col='Region', col_wrap=5, height=3.5, sharey=False)
g.map_dataframe(sns.barplot, x='Date', y='Unemployment_Rate', color='teal')
g.set_titles(col_template="{col_name}", size=9)

# Rotate x-axis labels
for ax in g.axes.flatten():
    ax.tick_params(axis='x', labelrotation=90)

g.fig.suptitle("Unemployment Rate by Region", fontsize=16)
plt.tight_layout()
plt.subplots_adjust(top=0.93)
plt.show()

"""TASK 3 - CAR PRICE PREDICTION WITH MACHINE LEARNING"""

# Step 1: Import libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Step 2: Load the dataset
df2 = pd.read_csv("/content/car_Data.csv")
df2.head()

# Step 3: Preprocessing
df2 = df2.copy()
df2['Car_Age'] = 2025 - df2['Year']  # Convert Year to Age
df2.drop(['Year', 'Car_Name'], axis=1, inplace=True)  # Drop unused columns

# Encode categorical columns
le = LabelEncoder()
df2['Fuel_Type'] = le.fit_transform(df2['Fuel_Type'])
df2['Selling_type'] = le.fit_transform(df2['Selling_type'])
df2['Transmission'] = le.fit_transform(df2['Transmission'])

# Confirm processed dataset
df2.head()

# Step 4: Define features and target
X = df2.drop('Selling_Price', axis=1)
y = df2['Selling_Price']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Train a Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Step 6: Evaluate the model
print("R² Score:", r2_score(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))

# Step 7: Visualization
plt.figure(figsize=(8,6))
sns.scatterplot(x=y_test, y=y_pred, color='crimson')
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Actual vs Predicted Car Prices")
plt.grid(True)
plt.show()

"""TASK 4 - SALES PREDICTION USING PYTHON DATASET"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Step 1: Load dataset
df3 = pd.read_csv("/content/Advertising.csv")
df3.head()

df3.drop('Unnamed: 0',axis=1, inplace=True)

# Step 2: Explore data
print(df3.head())
print(df3.describe())
print(df3.isnull().sum())

# Step 3: Visualize relationships
sns.pairplot(df3)
plt.show()

sns.heatmap(df3.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Step 4: Prepare data
X = df3.drop('Sales', axis=1)
y = df3['Sales']

# Step 5: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Train Linear Regression Model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 7: Predict & Evaluate
y_pred = model.predict(X_test)

print(f"R² Score: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")

# Step 8: Analyze coefficients (impact of ad spends)
coeff_df = pd.DataFrame(model.coef_, X.columns, columns=['Coefficient'])
print(coeff_df)

# Visualize impact of each advertising medium
coeff_df.plot(kind='bar', legend=False)
plt.title("Impact of Advertising Spend on Sales")
plt.ylabel("Coefficient Value")
plt.show()